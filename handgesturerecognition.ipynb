{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "import fnmatch\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,load_model,Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam,RMSprop,Nadam\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input,SeparableConv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from keras.applications import VGG19,VGG16,ResNet50,Xception\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "dict={7:\"ok\",6:\"index\",8:\"palm_moved\",0:\"down\",3:\"fist\",9:\"c\",2:\"l\",5:\"thumb\",4:\"fist_moved\",1:\"palm\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_x(d1):\n",
    "    x = []\n",
    "    for d2 in os.listdir(path+d1):\n",
    "        for root,dirs,files in os.walk(path+d1+'/'+d2):\n",
    "            for f in files:\n",
    "                if fnmatch.fnmatch(f,\"*.png\"):\n",
    "                    img = cv2.imread(path+d1+'/'+d2+'/'+f)\n",
    "                    img = cv2.resize(img,(224,224))\n",
    "                    x.append(img)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def extract_y(d1):\n",
    "    y = []\n",
    "    y1 = [] \n",
    "    for d2 in os.listdir(path+d1):\n",
    "        for root,dirs,files in os.walk(path+d1+'/'+d2):\n",
    "            for f in files:\n",
    "                y.append(d2[1])\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_Y = encoder.transform(y)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    y1 = np_utils.to_categorical(encoded_Y)\n",
    "    return y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    # (3) Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation before passing it to the next layer\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Passing it to a dense layer\n",
    "    model.add(Flatten())\n",
    "    # 1st Dense Layer\n",
    "    model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout to prevent overfitting\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Dense Layer\n",
    "    model.add(Dense(4096))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd Dense Layer\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # (4) Compile \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    filepath = \"RPS.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 7s 6ms/step - loss: 0.7862 - acc: 0.7433 - val_loss: 13.7908 - val_acc: 0.1100\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.11000, saving model to RPS.h5\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0912 - acc: 0.9725 - val_loss: 1.2779 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.11000 to 0.74375, saving model to RPS.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.5590 - acc: 0.8767 - val_loss: 6.3695 - val_acc: 0.3900\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.74375\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0457 - acc: 0.9858 - val_loss: 0.1320 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.74375 to 0.96000, saving model to RPS.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.9085 - acc: 0.8092 - val_loss: 2.6238 - val_acc: 0.6913\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.96000\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1765 - acc: 0.9442 - val_loss: 0.3114 - val_acc: 0.9350\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.96000\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3244 - acc: 0.9108 - val_loss: 0.6319 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.96000\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0270 - acc: 0.9908 - val_loss: 0.0144 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96000 to 0.99875, saving model to RPS.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2492 - acc: 0.9392 - val_loss: 2.0395 - val_acc: 0.7488\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.99875\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0737 - acc: 0.9758 - val_loss: 4.6639 - val_acc: 0.3962\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.99875\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0811 - acc: 0.9767 - val_loss: 0.0814 - val_acc: 0.9738\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.99875\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0097 - acc: 0.9975 - val_loss: 0.0374 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.99875\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0386 - acc: 0.9867 - val_loss: 0.7425 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.99875\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.0062 - val_acc: 0.9975\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.99875\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2522 - acc: 0.9342 - val_loss: 0.2364 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.99875\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0576 - acc: 0.9808 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.99875 to 1.00000, saving model to RPS.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.7787 - acc: 0.8608 - val_loss: 0.7238 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 1.00000\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0727 - acc: 0.9717 - val_loss: 0.2631 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2871 - acc: 0.9225 - val_loss: 0.0040 - val_acc: 0.9975\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 1.00000\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0161 - acc: 0.9958 - val_loss: 2.8327e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "path = \"../input/leapgestrecog/leapGestRecog/\"\n",
    "model, callbacks_list = model()\n",
    "dir = os.listdir(path)\n",
    "for d in dir:\n",
    "    x = []\n",
    "    y = []\n",
    "    x = extract_x(d)\n",
    "    y = extract_y(d)\n",
    "    x,y = shuffle(x,y,random_state=0)\n",
    "    lb=(int)(0.6*len(y))\n",
    "    train_x,valid_x = np.array(x[:lb]),np.array(x[lb:])\n",
    "    train_y,valid_y = np.array(y[:lb]),np.array(y[lb:])\n",
    "    history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=2, batch_size=32\n",
    "              ,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 1ms/step\n",
      "Loss:0.000\n"
     ]
    }
   ],
   "source": [
    "loss,mse = model.evaluate(x=valid_x, y=valid_y, batch_size=64, verbose=1, sample_weight=None, steps=None)\n",
    "print(\"Loss:{:.3f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
