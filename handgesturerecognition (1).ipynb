{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Neccesary Packagges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "import fnmatch\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,load_model,Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam,RMSprop,Nadam\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input,SeparableConv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from keras.applications import VGG19,VGG16,ResNet50,Xception\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "dict={7:\"ok\",6:\"index\",8:\"palm_moved\",0:\"down\",3:\"fist\",9:\"c\",2:\"l\",5:\"thumb\",4:\"fist_moved\",1:\"palm\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Extract Training Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_x(d1):\n",
    "    x = []\n",
    "    for d2 in os.listdir(path+d1):\n",
    "        for root,dirs,files in os.walk(path+d1+'/'+d2):\n",
    "            for f in files:\n",
    "                if fnmatch.fnmatch(f,\"*.png\"):\n",
    "                    img = cv2.imread(path+d1+'/'+d2+'/'+f)\n",
    "                    img = cv2.resize(img,(224,224))\n",
    "                    x.append(img)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To extract training Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def extract_y(d1):\n",
    "    y = []\n",
    "    y1 = [] \n",
    "    for d2 in os.listdir(path+d1):\n",
    "        for root,dirs,files in os.walk(path+d1+'/'+d2):\n",
    "            for f in files:\n",
    "                y.append(d2[1])\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_Y = encoder.transform(y)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    y1 = np_utils.to_categorical(encoded_Y)\n",
    "    return y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create AlexNet model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    # (3) Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation before passing it to the next layer\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Passing it to a dense layer\n",
    "    model.add(Flatten())\n",
    "    # 1st Dense Layer\n",
    "    model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout to prevent overfitting\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Dense Layer\n",
    "    model.add(Dense(4096))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd Dense Layer\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.4))\n",
    "    # Batch Normalisation\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # (4) Compile \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    filepath = \"Gesture.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 9s 7ms/step - loss: 0.5585 - acc: 0.8183 - val_loss: 14.4396 - val_acc: 0.1037\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.10375, saving model to Gesture.h5\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 3s 2ms/step - loss: 0.1389 - acc: 0.9583 - val_loss: 10.9099 - val_acc: 0.2612\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.10375 to 0.26125, saving model to Gesture.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.4188 - acc: 0.9067 - val_loss: 3.1236 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.26125 to 0.67250, saving model to Gesture.h5\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0541 - acc: 0.9833 - val_loss: 1.8046 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67250 to 0.75000, saving model to Gesture.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.9432 - acc: 0.7900 - val_loss: 3.5650 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.75000\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1977 - acc: 0.9367 - val_loss: 0.0692 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75000 to 0.98250, saving model to Gesture.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.2420 - acc: 0.9358 - val_loss: 3.0529 - val_acc: 0.6600\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.98250\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0701 - acc: 0.9800 - val_loss: 1.7182 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.98250\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3645 - acc: 0.9208 - val_loss: 4.1709 - val_acc: 0.5225\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.98250\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0584 - acc: 0.9783 - val_loss: 0.0807 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98250 to 0.98875, saving model to Gesture.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1253 - acc: 0.9567 - val_loss: 3.6016 - val_acc: 0.6288\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.98875\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1007 - acc: 0.9783 - val_loss: 0.0438 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98875 to 0.99500, saving model to Gesture.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0789 - acc: 0.9717 - val_loss: 5.4988 - val_acc: 0.5225\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.99500\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0101 - acc: 0.9958 - val_loss: 2.3676 - val_acc: 0.7087\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.99500\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.3002 - acc: 0.9350 - val_loss: 1.4269 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.99500\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0423 - acc: 0.9850 - val_loss: 9.7824e-05 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.99500 to 1.00000, saving model to Gesture.h5\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.6727 - acc: 0.8725 - val_loss: 1.1639 - val_acc: 0.7887\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 1.00000\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0794 - acc: 0.9750 - val_loss: 0.0794 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.1962 - acc: 0.9583 - val_loss: 0.5875 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 1.00000\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 2s 2ms/step - loss: 0.0284 - acc: 0.9917 - val_loss: 0.3857 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "path = \"../input/leapgestrecog/leapGestRecog/\"\n",
    "model, callbacks_list = model()\n",
    "dir = os.listdir(path)\n",
    "for d in dir:\n",
    "    x = []\n",
    "    y = []\n",
    "    x = extract_x(d)\n",
    "    y = extract_y(d)\n",
    "    x,y = shuffle(x,y,random_state=0)\n",
    "    lb=(int)(0.6*len(y))\n",
    "    train_x,valid_x = np.array(x[:lb]),np.array(x[lb:])\n",
    "    train_y,valid_y = np.array(y[:lb]),np.array(y[lb:])\n",
    "    history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=2, batch_size=32\n",
    "              ,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 464us/step\n",
      "Test loss: 0.3856910867476836\n",
      "Test accuracy: 94.75\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(valid_x, valid_y, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1]*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
